plugins {
  id 'com.linkedin.gradle.hadoop.HadoopPlugin'
}

// Simple positive test cases for the various types of jobs.

// Declare a couple jobs in global scope to check that these functions are working.

pigJob('pigJob1') {
  uses 'src/main/pig/pigScript.pig'
  reads files: [
    'path1' : '/user/foo'
  ]
  writes files: [
    'path2' : '/user/bar'
  ]
  set parameters: [
    'param1' : 'val1'
  ]
}

noOpJob('noOpJob1') {
  depends 'pigJob1'
}

// Now declare jobs for each of the job types to test them all out.

hadoop {
  buildPath "jobs"
  cleanPath false

  // Should not be built as this job is not within a workflow
  pigJob('pigJob0') {
    uses 'src/main/pig/pigScript.pig'
  }

  workflow('jobs1') {
    job('job1') {
    }

    job('job2') {
      reads files: [
        'foo' : '/data/databases/foo',
        'bar' : '/data/databases/bar',
      ]
      writes files: [
        'bazz' : '/data/databases/bazz'
      ]
      set properties: [
        'propertyName1' : 'propertyValue1'
      ]
      depends 'job1'
    }

    commandJob('job3') {
      uses 'echo "hello world"'
      depends clear: true, targetNames: ['job2']
    }

    hadoopJavaJob('job4') {
      uses 'com.linkedin.hello.HadoopJavaJob'
      caches files: [
        'foo.jar' : '/user/hacker/foo.jar'
      ]
      cachesArchive files: [
        'foobar' : '/user/hacker/foobar.zip'
      ]
      set confProperties: [
        'mapreduce.job.user.classpath.first': true,
        'mapreduce.reduce.memory.mb': 4096,
        'mapreduce.reduce.java.opts': '-Xmx3G'
      ]
      set properties: [
        'force.output.overwrite': true,
      ]
      queue 'marathon'
      depends 'job3'
    }

    hiveJob('job5') {
      uses "hello.q"
      reads files: [
        'path1' : '/user/foo'
      ]
      writes files: [
        'path2' : '/user/bar'
      ]
      set hadoopProperties: [
        'mapreduce.reduce.memory.mb': 4096,
        'mapreduce.reduce.java.opts': '-Xmx3G'
      ]
      set parameters: [
        'param1': 'val1',
        'param2': 'val2'
      ]
      queue 'marathon'
      depends 'job4'
    }

    javaJob('job6') {
      uses 'com.linkedin.hello.JavaJob'
      caches files: [
        'foo.jar' : '/user/hacker/foo.jar'
      ]
      cachesArchive files: [
        'foobar' : '/user/hacker/foobar.zip'
      ]
      set confProperties: [
        'mapreduce.job.user.classpath.first': true,
        'mapreduce.reduce.memory.mb': 4096
      ]
      // Test using both set confProperties and set hadoopProperties at the same time
      set hadoopProperties: [
        'mapreduce.reduce.memory.mb': 4096,
        'mapreduce.reduce.java.opts': '-Xmx3G'
      ]
      set properties: [
        'force.output.overwrite': true,
      ]
      queue 'marathon'
      // Test using the list syntax for the job depends method. This requires the use of parens.
      depends(['job5'])
    }

    javaProcessJob('job7') {
      uses 'com.linkedin.hello.JavaProcessJob'
      jvmClasspath './lib/*'
      set jvmProperties: [
        'jvmPropertyName1' : 'jvmPropertyValue1'
      ]
      Xms 128
      Xmx 1024
      depends 'job6'
    }

    kafkaPushJob('job8') {
      usesInputPath '/data/databases/MEMBER2/MEMBER_PROFILE/#LATEST'  // Required
      usesTopic 'kafkatestpush'                                       // Required
      usesBatchNumBytes 1000000                                       // Optional
      usesDisableSchemaRegistration true                              // Optional
      usesKafkaUrl 'theKafkaNode.linkedin.com:10251'                  // Optional
      usesNameNode 'hdfs://theNameNode.linkedin.com:9000'             // Optional
      usesSchemaRegistryUrl 'http://theKafkaNode:10252/schemaRegistry/schemas'  // Optional
      usesDisableAuditing true                                        // Optional
      usesMultiTopic true                                             // Optional
      usesWhitelistedTopics 'WhitelistedTopic.*'                        // Optional
      depends 'job7'
      queue 'marathon'
    }

    noOpJob('job9') {
      depends 'job8'
    }

    pigJob('job10') {
      uses 'src/main/pig/pigScript.pig'
      caches files: [
        'foo.jar' : '/user/hacker/foo.jar'
      ]
      cachesArchive files: [
        'foobar' : '/user/hacker/foobar.zip'
      ]
      set parameters: [
        'param1' : 'val1'
      ]
      queue 'marathon'
      depends 'job9'
    }

    sparkJob('job11') {
      uses 'com.linkedin.hello.spark.HelloSpark' // required
      executes 'hello-spark.jar' // required

      def params = ['param1','param2']
      def flags = ['verbose','version']

      appParams params // optional
      enableFlags flags // optional

      set sparkConfs: [
        'key1': 'value1',
        'key2': 'value2'
      ]

      jars 'jar1,jar2,jar3'
      numExecutors 120
      executorMemory '2g'
      driverMemory '2g'
      executorCores 1

      set properties: [
        'master':'yarn-cluster',
      ]
      set jvmProperties: [
        'log4j.configuration' : 'log.properties'
      ]
      queue 'marathon'
      depends 'job10'
    }

    sparkJob('job11RequiredJar') {
      uses 'com.linkedin.hello.spark.HelloSpark' // conditional
      executes 'hello-spark.jar' // required
      depends 'job11'
    }

    sparkJob('job11RequiredPy') {
      executes 'hello-pyspark.py' //required
      depends 'job11RequiredJar'
    }

    voldemortBuildPushJob('job12') {
      usesStoreName 'test-store'          // Required
      usesClusterName 'tcp://foo:10103'   // Required
      usesInputPath '/user/foo/input'     // Required
      usesOutputPath '/user/foo/output'   // Required
      usesStoreOwners 'foo@linkedin.com'  // Required
      usesStoreDesc 'Store for testing'   // Required
      usesTempDir '/tmp/foo'              // Optional
      usesRepFactor 2                     // Optional
      usesCompressValue false             // Optional
      usesKeySelection 'memberId'         // Optional
      usesValueSelection 'lastName'       // Optional
      usesNumChunks(-1)                   // Optional
      usesChunkSize 1073741824            // Optional
      usesKeepOutput false                // Optional
      usesPushHttpTimeoutSeconds 86400    // Optional
      usesPushNode 0                      // Optional
      usesBuildStore true                 // Optional
      usesPushStore true                  // Optional
      usesFetcherProtocol 'hftp'          // Optional
      usesFetcherPort '50070'             // Optional
      usesAvroSerializerVersioned false   // Optional
      usesAvroData true                   // Optional
      usesAvroKeyField 'memberId'         // Optional unless isAvroData is true
      usesAvroValueField 'firstName'      // Optional unless isAvroData is true
      depends 'job11RequiredPy'
      queue 'marathon'
    }

    hadoopShellJob('job13') {
      uses 'echo "hello world"'
      depends clear: true, targetNames: ['job12']
    }

    hdfsToTeradataJob('job14') {
      hostName 'dw.teradata.com'                         //Required
      userId 'scott'                                     //Required
      encryptedCredential 'eyJ2YWwiOiJiQzVCU09HbDVwYndxNFRXV00yZ253QUN0Z3E5SnlMMjhMV'  //Required
      cryptoKeyFilePath '/hdfs/path/crypto/key/file'     //Required
      sourceHdfsPath '/data/test/users/daily/2015-10-07' //Required
      targetTable 'dwh_stg.scott_users'                  //Required
      avroSchemaPath '/data/test/users/user.avsc'
      set properties: [                                  //Optional
        "user.to.proxy": "scott"
      ]
      set hadoopProperties: [                            //Optional
        "mapreduce.map.child.java.opts": "'-Xmx1G -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true'",
        "mapreduce.job.user.classpath.first": true
      ]
      depends 'job13'
    }

    teradataToHdfsJob('job15') {
      hostName 'dw.teradata.com'                         //Required
      userId 'scott'                                     //Required
      encryptedCredential 'eyJ2YWwiOiJiQzVCU09HbDVwYndxNFRXV00yZ253QUN0Z3E5SnlMMjhMV'   //Required
      cryptoKeyFilePath '/hdfs/path/crypto/key/file'     //Required
      sourceTable 'dwh_stg.scott_users'                //Required
      targetHdfsPath '/data/test/users/teradata_to_hdfs_test' //Required
      avroSchemaPath '/data/test/users/user.avsc'
      set properties: [                                  //Optional
        "user.to.proxy": "scott",
        "force.output.overwrite": true
      ]
      set hadoopProperties: [                            //Optional
        "mapreduce.map.child.java.opts": "'-Xmx1G -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true'",
        "mapreduce.job.user.classpath.first": true
      ]
      depends 'job14'
    }

    hdfsToEspressoJob('job16') {
      qps 1000                                                               //Required
      sourceHdfsPath '/jobs/merlin2/accountMetrics/espressoStaging/shortest' //Required
      espressoEndpoint 'http://eat1-app1237.stg.linkedin.com:11936'          //Required
      espressoDatabaseName 'MerlinAccounts'                                  //Required
      espressoTableName 'AccountMetric'                                      //Required
      espressoContentType 'APPLICATION_JSON'                                 //Required
      espressoKey 'accountId'                                                //Required
      espressoSubkeys 'metricName'                                           //Optional
      espressoOperation 'put'                                                //Required
      errorHdfsPath '/jobs/merlin2/accountMetrics/espressoStaging/error'     //Required
      set properties: [                                                      //Optional
        'force.error.overwrite' : true
      ]
      depends 'job15'
    }

    gobblinJob('job17') {
      workDir '/jobs/jnchang/azkaban/gobblin'  //Optional
      preset 'mysqlToHdfs'                     //Optional
      set properties: [                        //Optional
        'source.querybased.schema' : 'DATABASE',
        'source.entity' : 'user',
        'source.conn.host' : 'MYSQL.HOST.com',
        'source.conn.username' : 'USER_NAME',
        'source.conn.password' : 'ENCRYPTED_CREDENTIAL',
        'encrypt.key.loc' : '/path/to/key',
        'extract.table.type' : 'snapshot_only',
        'extract.is.full' : true,
        'data.publisher.replace.final.dir' : true,
        'data.publisher.final.dir' :  '${gobblin.work_dir}/job-output'
      ]
      depends 'job16'
    }

    sqlJob('job18') {
      jdbcUrl '/job/data/src'            //Required
      jdbcUserId 'foo'                   //Required
      jdbcEncryptedCredential 'eyJ2YWwiOiJiQzVCU09HbDVwYndxNFRXV00yZ'  //Required
      jdbcCryptoKeyPath '/hdfs/path/to/cryptokey/file'                 //Required
      set properties: [
        'user.to.proxy' : 'testUser',
        'jdbc.sql.1' : 'DELETE test_table_publish ALL;',
        'jdbc.sql.2' : 'INSERT INTO test_table_publish SELECT * FROM test_table;'
      ]
      depends 'job17'
    }

    hdfsToTeradataJob('job19') {
      hostName 'dw.teradata.com'  // Required
      userId 'foo' //Required
      cryptoKeyFilePath '/hdfs/path/to/cryptokey/file'
      encryptedCredential 'eyJ2YWwiOiJiQzVCU09HbDVwYndxNFRXV00yZ'
      targetTable 'dwh_stg.scott_users'
      sourceHiveDatabase 'hive_database'
      sourceHiveTable 'hive_table'
      set properties: [
        "user.to.proxy": "testUser",
        "hdfs.fileformat": "orcfile"
      ]
      depends 'job18'
    }

    pinotBuildAndPushJob('job20') {
      usesTableName 'internalTesting' // Required
      usesInputPath '/user/input' // Required
      usesPushLocation 'host:port' // Required
      depends 'job19'
    }

    tableauJob('job21') {
      usesWorkbookName 'workbookName' // Required
      depends 'job20'
    }

    hdfsWaitJob('job22') {
      jvmClasspath './lib/*'
      directoryPath 'foo'      // Required
      directoryFreshness '10D' // Required
      timeoutAfter '20M 1H'    // Required
      sleepTime '2M 3S'
      checkExactPath true
      timezone 'America/Los_Angeles'
      failOnTimeout true       // Required
      depends 'job21'
    }

    venicePushJob('job23') {
      usesAvroKeyField 'key'
      usesAvroValueField 'value'
      usesClusterName '${venice.cluster.0}'
      usesInputPath '/user/voldtest/h2v_test/1k'
      usesVeniceStoreName 'venice_h2v_test_1k'
      depends 'job22'
    }

    tensorFlowJob('job24', 'tony') {
      executes 'path/to/python/script.py'
      amMemory '2g'
      amCores 1
      amGpus 1
      psMemory '2g'
      psCores 1
      workerMemory '8g'
      workerCores 1
      workerGpus 2
      numPs 2
      numWorkers 4
      archive 'tensorflow-starter-kit.zip'
      set workerEnv: [
        'ENV1': 'val1',
        'ENV2': 'val2'
      ]
      depends 'job23'
    }

    tensorFlowJob('job25', 'spark') {
      executes 'path/to/python/script.py'
      amMemory '2g'
      amCores 1
      // Put ps memory before worker memory and ps cores after worker cores
      // to verify only worker params take effect
      psMemory '2g'
      workerCores 2
      workerMemory '8g'
      psCores 1
      numPs 2
      numWorkers 4
      queue 'marathon'
      set sparkConfs: [
        'key1': 'value1',
        'key2': 'value2'
      ]
      depends 'job24'
    }

    tonyJob('job25a') {
      set properties: [
        'executes': 'path/to/python/script.py',
        'tony.ps.vcores': 1,
        'tony.ps.memory': '2g',
        'tony.ps.instances': 2,
        'tony.worker.vcores': 1,
        'tony.worker.memory': '8g',
        'tony.worker.gpus': 2,
        'tony.worker.instances': 4,
      ]
      depends 'job25'
    }

    wormholePushJob('job26') {
      fromInputPath '/user/input'            // Required
      toNamespace 'linkedin'                 // Required
      toDataset 'model'                      // Required
      withVersionString '1.2.4-2018.09.04'   // Optional
      preserveInput true                     // Optional
      toOutputLocation 'LOCAL'               // Optional

      depends 'job25a'
    }

    kabootarJob('job27') {
      usesTrainedModelLocation '/user/testmodelregsvc/trained-models'  // Required
      usesTrainingName 'AyeAyeCaptain'                                 // Required
      usesAiProjectGroup 'AIFoundationOther'                           // Required
      usesWormholeNamespace 'testmodelregsvc'                          // Required
      usesInitialImport 'initial/import/File'                          // Required
      usesTrainingID '53fe1ff5-4439-4288-be43-11cb11629552'            // Optional
      usesOrigin 'FELLOWSHIP'                                          // Optional
      usesFramework 'PHOTON_CONNECT'                                   // Optional
      usesModelSupplementaryDataLocation '/user/testmodelregsvc/data'  // Optional
      usesEnableQuasarModelBundle true                                 // Optional
      usesEnableAutoPublish true                                       // Optional
      usesAutoPublishModelName 'myFirstModel'                          // Required if usesEnableAutoPublish is true
      usesAutoPublishModelDeploymentGroupName 'myDeploymentGroup'      // Required if usesEnableAutoPublish is true
      usesAutoPublishVersionUpdateType 'PATCH'                         // Optional
      usesAutoPublishModelContainsPiiData false                        // Required if usesEnableAutoPublish is true
      usesAutoPublishModelContainsConfidentialData false               // Required if usesEnableAutoPublish is true

      depends 'job26'
    }

    wormholePushJob2('job28') {
      fromInputPath '/user/input'            // Required
      toNamespace 'linkedin'                 // Required
      toDataset 'model'                      // Required
      withVersionString '1.2.4-2019.09.04'   // Optional
      preserveInput true                     // Optional
      toOutputLocation 'LOCAL'               // Optional

      depends 'job27'
    }

    kubernetesJob('job29') {
      kind 'MPIJob'                                 // Required
      namespace 'some-namespace'                    // Optional
      set properties: [
          'k8s.launcher.instances': 1,              // Optional
          'k8s.worker.instances': 1,                // Optional
          'k8s.worker.gpus': 4,                     // Optional
          'k8s.containers.image': 'some.image.url'  // Optional
      ]
      depends 'job28'
    }

    carbonJob('job30') {
      taskType 'retention'   // Required

      depends 'job29'
    }

    targets 'job29'
  }
}
