buildscript {
  dependencies {
    classpath files("${project.pluginTestDir}/hadoop-plugin-${project.version}.jar", "${project.pluginTestDir}/hadoop-plugin-${project.version}-SNAPSHOT.jar")
  }
}

apply plugin: com.linkedin.gradle.hadoop.HadoopPlugin

// Simple positive test cases for the various types of jobs.

// Declare a couple jobs in global scope to check that these functions are working.

pigJob('pigJob1') {
  uses 'src/main/pig/pigScript.pig'
  reads files: [
    'path1' : '/user/foo'
  ]
  writes files: [
    'path2' : '/user/bar'
  ]
  set parameters: [
    'param1' : 'val1'
  ]
}

noOpJob('noOpJob1') {
  depends 'pigJob1'
}

// Now declare jobs for each of the job types to test them all out.

hadoop {
  buildPath "jobs"
  cleanPath false

  // Should not be built as this job is not within a workflow
  pigJob('pigJob0') {
    uses 'src/main/pig/pigScript.pig'
  }

  workflow('jobs1') {
    job('job1') {
    }

    job('job2') {
      reads files: [
        'foo' : '/data/databases/foo',
        'bar' : '/data/databases/bar',
      ]
      writes files: [
        'bazz' : '/data/databases/bazz'
      ]
      set properties: [
        'propertyName1' : 'propertyValue1'
      ]
      depends 'job1'
    }

    commandJob('job3') {
      uses 'echo "hello world"'
      depends clear: true, targetNames: ['job2']
    }

    hadoopJavaJob('job4') {
      uses 'com.linkedin.hello.HadoopJavaJob'
      caches files: [
        'foo.jar' : '/user/hacker/foo.jar'
      ]
      cachesArchive files: [
        'foobar' : '/user/hacker/foobar.zip'
      ]
      set confProperties: [
        'mapreduce.job.user.classpath.first': true,
        'mapreduce.reduce.memory.mb': 4096,
        'mapreduce.reduce.java.opts': '-Xmx3G'
      ]
      set properties: [
        'force.output.overwrite': true,
      ]
      queue 'marathon'
      depends 'job3'
    }

    hiveJob('job5') {
      uses "hello.q"
      reads files: [
        'path1' : '/user/foo'
      ]
      writes files: [
        'path2' : '/user/bar'
      ]
      set hadoopProperties: [
        'mapreduce.reduce.memory.mb': 4096,
        'mapreduce.reduce.java.opts': '-Xmx3G'
      ]
      set parameters: [
        'param1': 'val1',
        'param2': 'val2'
      ]
      queue 'marathon'
      depends 'job4'
    }

    javaJob('job6') {
      uses 'com.linkedin.hello.JavaJob'
      caches files: [
        'foo.jar' : '/user/hacker/foo.jar'
      ]
      cachesArchive files: [
        'foobar' : '/user/hacker/foobar.zip'
      ]
      set confProperties: [
        'mapreduce.job.user.classpath.first': true,
        'mapreduce.reduce.memory.mb': 4096
      ]
      // Test using both set confProperties and set hadoopProperties at the same time
      set hadoopProperties: [
        'mapreduce.reduce.memory.mb': 4096,
        'mapreduce.reduce.java.opts': '-Xmx3G'
      ]
      set properties: [
        'force.output.overwrite': true,
      ]
      queue 'marathon'
      // Test using the list syntax for the job depends method. This requires the use of parens.
      depends(['job5'])
    }

    javaProcessJob('job7') {
      uses 'com.linkedin.hello.JavaProcessJob'
      jvmClasspath './lib/*'
      set jvmProperties: [
        'jvmPropertyName1' : 'jvmPropertyValue1'
      ]
      Xms 128
      Xmx 1024
      depends 'job6'
    }

    kafkaPushJob('job8') {
      usesInputPath '/data/databases/MEMBER2/MEMBER_PROFILE/#LATEST'  // Required
      usesTopic 'kafkatestpush'                                       // Required
      usesBatchNumBytes 1000000                                       // Optional
      usesDisableSchemaRegistration true                              // Optional
      usesKafkaUrl 'theKafkaNode.linkedin.com:10251'                  // Optional
      usesNameNode 'hdfs://theNameNode.linkedin.com:9000'             // Optional
      usesSchemaRegistryUrl 'http://theKafkaNode:10252/schemaRegistry/schemas'  // Optional
      usesDisableAuditing true                                        // Optional
      usesMultiTopic true                                             // Optional
      usesWhitelistedTopics 'WhitelistedTopic.*'                        // Optional
      depends 'job7'
      queue 'marathon'
    }

    noOpJob('job9') {
      depends 'job8'
    }

    pigJob('job10') {
      uses 'src/main/pig/pigScript.pig'
      caches files: [
        'foo.jar' : '/user/hacker/foo.jar'
      ]
      cachesArchive files: [
        'foobar' : '/user/hacker/foobar.zip'
      ]
      set parameters: [
        'param1' : 'val1'
      ]
      queue 'marathon'
      depends 'job9'
    }

    sparkJob('job11') {
      uses 'com.linkedin.hello.spark.HelloSpark' // required
      executes 'hello-spark.jar' // required

      def params = ['param1','param2']
      def flags = ['verbose','version']

      appParams params // optional
      enableFlags flags // optional

      set sparkConfs: [
        'key1': 'value1',
        'key2': 'value2'
      ]

      jars 'jar1,jar2,jar3'
      numExecutors 120
      executorMemory '2g'
      driverMemory '2g'
      executorCores 1

      set properties: [
        'master':'yarn-cluster',
      ]
      set jvmProperties: [
        'log4j.configuration' : 'log.properties'
      ]
      queue 'marathon'
      depends 'job10'
    }

    sparkJob('job11RequiredJar') {
      uses 'com.linkedin.hello.spark.HelloSpark' // conditional
      executes 'hello-spark.jar' // required
      depends 'job11'
    }

    sparkJob('job11RequiredPy') {
      executes 'hello-pyspark.py' //required
      depends 'job11RequiredJar'
    }

    voldemortBuildPushJob('job12') {
      usesStoreName 'test-store'          // Required
      usesClusterName 'tcp://foo:10103'   // Required
      usesInputPath '/user/foo/input'     // Required
      usesOutputPath '/user/foo/output'   // Required
      usesStoreOwners 'foo@linkedin.com'  // Required
      usesStoreDesc 'Store for testing'   // Required
      usesTempDir '/tmp/foo'              // Optional
      usesRepFactor 2                     // Optional
      usesCompressValue false             // Optional
      usesKeySelection 'memberId'         // Optional
      usesValueSelection 'lastName'       // Optional
      usesNumChunks(-1)                   // Optional
      usesChunkSize 1073741824            // Optional
      usesKeepOutput false                // Optional
      usesPushHttpTimeoutSeconds 86400    // Optional
      usesPushNode 0                      // Optional
      usesBuildStore true                 // Optional
      usesPushStore true                  // Optional
      usesFetcherProtocol 'hftp'          // Optional
      usesFetcherPort '50070'             // Optional
      usesAvroSerializerVersioned false   // Optional
      usesAvroData true                   // Optional
      usesAvroKeyField 'memberId'         // Optional unless isAvroData is true
      usesAvroValueField 'firstName'      // Optional unless isAvroData is true
      depends 'job11RequiredPy'
      queue 'marathon'
    }

    hadoopShellJob('job13') {
      uses 'echo "hello world"'
      depends clear: true, targetNames: ['job12']
    }

    hdfsToTeradataJob('job14') {
      hostName 'dw.teradata.com'                         //Required
      userId 'scott'                                     //Required
      encryptedCredential 'eyJ2YWwiOiJiQzVCU09HbDVwYndxNFRXV00yZ253QUN0Z3E5SnlMMjhMV'  //Required
      cryptoKeyFilePath '/hdfs/path/crypto/key/file'     //Required
      sourceHdfsPath '/data/test/users/daily/2015-10-07' //Required
      targetTable 'dwh_stg.scott_users'                  //Required
      avroSchemaPath '/data/test/users/user.avsc'
      set properties: [                                  //Optional
        "user.to.proxy": "scott"
      ]
      set hadoopProperties: [                            //Optional
        "mapreduce.map.child.java.opts": "'-Xmx1G -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true'",
        "mapreduce.job.user.classpath.first": true
      ]
      depends 'job13'
    }

    teradataToHdfsJob('job15') {
      hostName 'dw.teradata.com'                         //Required
      userId 'scott'                                     //Required
      encryptedCredential 'eyJ2YWwiOiJiQzVCU09HbDVwYndxNFRXV00yZ253QUN0Z3E5SnlMMjhMV'   //Required
      cryptoKeyFilePath '/hdfs/path/crypto/key/file'     //Required
      sourceTable 'dwh_stg.scott_users'                //Required
      targetHdfsPath '/data/test/users/teradata_to_hdfs_test' //Required
      avroSchemaPath '/data/test/users/user.avsc'
      set properties: [                                  //Optional
        "user.to.proxy": "scott",
        "force.output.overwrite": true
      ]
      set hadoopProperties: [                            //Optional
        "mapreduce.map.child.java.opts": "'-Xmx1G -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true'",
        "mapreduce.job.user.classpath.first": true
      ]
      depends 'job14'
    }

    hdfsToEspressoJob('job16') {
      qps 1000                                                               //Required
      sourceHdfsPath '/jobs/merlin2/accountMetrics/espressoStaging/shortest' //Required
      espressoEndpoint 'http://eat1-app1237.stg.linkedin.com:11936'          //Required
      espressoDatabaseName 'MerlinAccounts'                                  //Required
      espressoTableName 'AccountMetric'                                      //Required
      espressoContentType 'APPLICATION_JSON'                                 //Required
      espressoKey 'accountId'                                                //Required
      espressoSubkeys 'metricName'                                           //Optional
      espressoOperation 'put'                                                //Required
      errorHdfsPath '/jobs/merlin2/accountMetrics/espressoStaging/error'     //Required
      set properties: [                                                      //Optional
        'force.error.overwrite' : true
      ]
      depends 'job15'
    }

    gobblinJob('job17') {
      workDir '/jobs/jnchang/azkaban/gobblin'  //Optional
      preset 'mysqlToHdfs'                     //Optional
      set properties: [                        //Optional
        'source.querybased.schema' : 'DATABASE',
        'source.entity' : 'user',
        'source.conn.host' : 'MYSQL.HOST.com',
        'source.conn.username' : 'USER_NAME',
        'source.conn.password' : 'ENCRYPTED_CREDENTIAL',
        'encrypt.key.loc' : '/path/to/key',
        'extract.table.type' : 'snapshot_only',
        'extract.is.full' : true,
        'data.publisher.replace.final.dir' : true,
        'data.publisher.final.dir' :  '${gobblin.work_dir}/job-output'
      ]
      depends 'job16'
    }

    sqlJob('job18') {
      jdbcDriverClass 'com.teradata.jdbc.TeraDriver' //Required
      jdbcUrl '/job/data/src'            //Required
      jdbcUserId 'foo'                   //Required
      jdbcEncryptedCredential 'eyJ2YWwiOiJiQzVCU09HbDVwYndxNFRXV00yZ'  //Required
      jdbcCryptoKeyPath '/hdfs/path/to/cryptokey/file'                 //Required
      set properties: [
        'user.to.proxy' : 'testUser',
        'jdbc.sql.1' : 'DELETE test_table_publish ALL;',
        'jdbc.sql.2' : 'INSERT INTO test_table_publish SELECT * FROM test_table;'
      ]
      depends 'job17'
    }

    hdfsToTeradataJob('job19') {
      hostName 'dw.teradata.com'  // Required
      userId 'foo' //Required
      cryptoKeyFilePath '/hdfs/path/to/cryptokey/file'
      encryptedCredential 'eyJ2YWwiOiJiQzVCU09HbDVwYndxNFRXV00yZ'
      targetTable 'dwh_stg.scott_users'
      sourceHiveDatabase 'hive_database'
      sourceHiveTable 'hive_table'
      set properties: [
        "user.to.proxy": "testUser",
        "hdfs.fileformat": "orcfile"
      ]
      depends 'job18'
    }

    pinotBuildAndPushJob('job20') {
      usesTableName 'internalTesting' // Required
      usesInputPath '/user/input' // Required
      usesPushLocation 'host:port' // Required
      depends 'job19'
    }

    tableauJob('job21') {
      usesWorkbookName 'workbookName' // Required
      depends 'job20'
    }

    hdfsWaitJob('job22') {
      jvmClasspath './lib/*'
      directoryPath 'foo'      // Required
      directoryFreshness '10D' // Required
      timeoutAfter '20M 1H'    // Required
      sleepTime '2M 3S'
      checkExactPath true
      timezone 'America/Los_Angeles'
      failOnTimeout true       // Required
      depends 'job21'
    }

    venicePushJob('job23') {
      usesAvroKeyField 'key'
      usesAvroValueField 'value'
      usesClusterName '${venice.cluster.0}'
      usesInputPath '/user/voldtest/h2v_test/1k'
      usesVeniceStoreName 'venice_h2v_test_1k'
      depends 'job22'
    }

    tensorFlowJob('job24') {
      amMemory 2048
      amCores 1
      psMemory 2048
      psCores 1
      workerMemory 8192
      workerCores 1
      workerGpus 2
      numPs 2
      numWorkers 4
      archive 'tensorflow-starter-kit.zip'
      jar 'tensorflow-on-yarn-0.0.1.jar'
      taskCommand './tf.py'
      set workerEnv: [
        'ENV1': 'val1',
        'ENV2': 'val2'
      ]
      depends 'job23'
    }
    targets 'job24'
  }
}
